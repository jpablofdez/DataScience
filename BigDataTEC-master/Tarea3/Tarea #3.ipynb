{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BigData\n",
    "\n",
    "Tarea #3\n",
    "\n",
    "Juan Pablo Fernández Deltado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "En la presente tarea, se realiza una conexión con la base de datos postgres y un join con un archivo csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Para realizar la conexión con la base de datos se agrego el driver de conexión de postgre en la siguiente ruta:\n",
    "C:\\Spark\\jars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerias de pyspark\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuracion del driver de conexión de postgres\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"C:/Users/JuanPablo/BigDataTEC-master/Tarea3/postgresql-42.2.9.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"C:/Users/JuanPablo/BigDataTEC-master/Tarea3/postgresql-42.2.9.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parametros de conexión y lectura de tabla de la base de datos\n",
    "\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/postgres\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# se agrega una columna nueva de la fecha con su respectivo formato\n",
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|      date|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|2017-03-01|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|2017-03-02|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|2017-03-02|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|2017-03-03|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|2017-03-01|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|2017-03-01|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|2017-03-03|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|2017-03-03|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# se agrega una columna nueva de fecha\n",
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date, '%m/%d/%Y'),\n",
    "        DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n",
    "typed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+-----------+\n",
      "|customer_id|      date|sum(id)|sum(customer_id)|sum(amount)|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "|          2|2017-03-01|     21|               6|       1000|\n",
      "|          1|2017-03-02|      7|               2|         96|\n",
      "|          1|2017-03-03|      5|               1|        128|\n",
      "|          1|2017-03-01|      3|               2|        180|\n",
      "|          2|2017-03-03|     19|               4|         55|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se suman los datos por medio de la funcion groupby\n",
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \"C:\\Spark\\postgresql-42.2.9.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/postgres\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"admin\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|          2|2017-03-01|  1000|\n",
      "|          1|2017-03-02|    96|\n",
      "|          1|2017-03-03|   128|\n",
      "|          1|2017-03-01|   180|\n",
      "|          2|2017-03-03|    55|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+---+----+--------+\n",
      "| id|name|currency|\n",
      "+---+----+--------+\n",
      "|  1|John|     CRC|\n",
      "|  2|Jane|     EUR|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cargamos una fuente de datos en un archivo csv\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "names_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"names.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"id\", IntegerType()),\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"currency\", StringType())])) \\\n",
    "    .load()\n",
    "\n",
    "names_df.printSchema()\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|customer_id|      date|amount| id|name|currency|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|          2|2017-03-01|  1000|  2|Jane|     EUR|\n",
      "|          1|2017-03-02|    96|  1|John|     CRC|\n",
      "|          1|2017-03-03|   128|  1|John|     CRC|\n",
      "|          1|2017-03-01|   180|  1|John|     CRC|\n",
      "|          2|2017-03-03|    55|  2|Jane|     EUR|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unimos las tablas de las diferentes fuentes de datos, tanto de la base de datos con del csv\n",
    "\n",
    "joint_df = stats_df.join(names_df, stats_df.customer_id == names_df.id)\n",
    "joint_df.printSchema()\n",
    "joint_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
